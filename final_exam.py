# -*- coding: utf-8 -*-
"""final exam.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eEqKDm3NNMmj0MdmUX8y1ohHkmd7hqKa
"""

# ========================================================
# ðŸ“¦ STEP 1 â€” Import Libraries
# ========================================================
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import seaborn as sns

# ========================================================
# ðŸ“‚ STEP 2 â€” Load Dataset
# ========================================================
data = load_breast_cancer()
X = data.data                 # Features
y = data.target               # Labels (0 = malignant, 1 = benign)

# ========================================================
# ðŸ“Š STEP 3 â€” Data Exploration
# ========================================================
print("Shape of X (features):", X.shape)
print("Shape of y (labels):", y.shape)

# Convert to DataFrame
df = pd.DataFrame(X, columns=data.feature_names)
df['target'] = y

print("\nFirst 5 rows:")
print(df.head())

print("\nSummary Statistics:")
print(df.describe())

print("\nClass Distribution:")
print(df['target'].value_counts())

# ========================================================
# ðŸ”€ STEP 4 â€” Trainâ€“Test Split
# ========================================================
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# ========================================================
# ðŸ“‰ STEP 5 â€” PCA + ML Pipeline
# ========================================================
pca_components = 10   # keep top 10 components

pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('pca', PCA(n_components=pca_components)),
])

# Fit PCA on training data
X_train_pca = pipeline.fit_transform(X_train)
X_test_pca = pipeline.transform(X_test)

# ========================================================
# ðŸ¤– STEP 6 â€” Train Classifier (Logistic Regression)
# ========================================================
from sklearn.linear_model import LogisticRegression

model = LogisticRegression(max_iter=500)
model.fit(X_train_pca, y_train)

# ========================================================
# ðŸ“ˆ STEP 7 â€” Predictions & Evaluation
# ========================================================
y_pred = model.predict(X_test_pca)

print("\nAccuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

print("\nConfusion Matrix:")
cm = confusion_matrix(y_test, y_pred)
print(cm)

# Confusion Matrix Heatmap
plt.figure(figsize=(6,4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

# ========================================================
# ðŸ“ˆ STEP 8 â€” PCA Explained Variance Plot
# ========================================================
pca_model = pipeline.named_steps['pca']

plt.figure(figsize=(8,5))
plt.plot(np.cumsum(pca_model.explained_variance_ratio_), marker='o')
plt.title("Cumulative Explained Variance by PCA Components")
plt.xlabel("Number of Components")
plt.ylabel("Cumulative Explained Variance")
plt.grid(True)
plt.show()


# ========================================================
# â­ ADVANCED PCA ANALYSIS (3 Marks)
# ========================================================
print("\n========= ADVANCED PCA ANALYSIS =========")

# Standardize all data before full PCA
scaler_full = StandardScaler()
X_scaled_full = scaler_full.fit_transform(X)

# Apply PCA without limiting components
pca_full = PCA()
pca_full.fit(X_scaled_full)

# Cumulative variance
cum_variance = np.cumsum(pca_full.explained_variance_ratio_)

# Components needed for 95% variance
components_95 = np.argmax(cum_variance >= 0.95) + 1

# Components needed for 99% variance
components_99 = np.argmax(cum_variance >= 0.99) + 1

print(f"Number of components for 95% variance: {components_95}")
print(f"Number of components for 99% variance: {components_99}")

# OPTIONAL: Plot cumulative variance for all components
plt.figure(figsize=(8,5))
plt.plot(cum_variance, marker='o')
plt.axhline(0.95, color='r', linestyle='--', label="95% variance")
plt.axhline(0.99, color='g', linestyle='--', label="99% variance")
plt.title("Full PCA Cumulative Explained Variance")
plt.xlabel("Number of Components")
plt.ylabel("Cumulative Variance Explained")
plt.legend()
plt.grid(True)
plt.show()

# ================================
# ðŸ“Œ STEP 1 â€” Import Libraries
# ================================
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import load_breast_cancer

# ================================
# ðŸ“Œ STEP 2 â€” Load Your Dataset
# ================================
data = load_breast_cancer()
df = pd.DataFrame(data=data.data, columns=data.feature_names)
df["diagnosis"] = data.target  # 0 = malignant, 1 = benign

print(df.head())

# ================================
# ðŸ“Œ STEP 3 â€” Split Features & Target
# ================================
X = df.drop(columns=["diagnosis"])
y = df["diagnosis"]

# ================================
# ðŸ“Œ STEP 4 â€” Standardize All Features
# ================================
scaler = StandardScaler()

# ================================
# ðŸ“Œ STEP 5 â€” Stratified 80/20 Split
# ================================
X_train, X_test, y_train, y_test = train_test_split(
    X,
    y,
    test_size=0.20,
    stratify=y,     # preserves class ratio
    random_state=42
)

# Fit scaler ONLY on training data (avoids data leakage)
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ================================
# ðŸ“Œ FINAL OUTPUT
# ================================
print("Training shape:", X_train_scaled.shape)
print("Testing shape:", X_test_scaled.shape)
print("Class distribution in training set:\n", y_train.value_counts())
print("Class distribution in testing set:\n", y_test.value_counts())

# ================================
# STEP 1 â€” Import Libraries
# ================================
import pandas as pd
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# ================================
# STEP 2 â€” Load Dataset
# ================================
data = load_breast_cancer()
X = data.data
y = data.target
feature_names = data.feature_names

# Convert to DataFrame for better readability
df = pd.DataFrame(X, columns=feature_names)
df['target'] = y

# ================================
# STEP 3 â€” Train-Test Split
# ================================
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ================================
# STEP 4 â€” Standardize Data
# ================================
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ================================
# STEP 5 â€” PCA Analysis
# ================================

# PCA to determine explained variance
pca_full = PCA()
pca_full.fit(X_train_scaled)

# Calculate cumulative variance
cumulative_variance = pca_full.explained_variance_ratio_.cumsum()

# Number of components for 95% variance
n_components_95 = next(i for i, total_var in enumerate(cumulative_variance) if total_var >= 0.95) + 1
print(f"Number of components for 95% variance: {n_components_95}")

# Number of components for 99% variance
n_components_99 = next(i for i, total_var in enumerate(cumulative_variance) if total_var >= 0.99) + 1
print(f"Number of components for 99% variance: {n_components_99}")

# ================================
# STEP 6 â€” Transform Training & Testing Sets using 95% variance PCA
# ================================
pca_95 = PCA(n_components=n_components_95)
X_train_pca = pca_95.fit_transform(X_train_scaled)
X_test_pca = pca_95.transform(X_test_scaled)

print(f"Shape of original X_train: {X_train_scaled.shape}")
print(f"Shape of X_train after PCA (95% variance): {X_train_pca.shape}")

# ============================================================
# STEP 1 â€” Import Libraries
# ============================================================
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_breast_cancer
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# ============================================================
# STEP 2 â€” Load Dataset
# ============================================================
data = load_breast_cancer()
X = data.data
y = data.target
feature_names = data.feature_names

# ============================================================
# STEP 3 â€” Standardize the Data
# ============================================================
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# ============================================================
# STEP 4 â€” Apply PCA
# ============================================================
pca = PCA()
X_pca = pca.fit_transform(X_scaled)

explained_variance_ratio = pca.explained_variance_ratio_
cumulative_explained_variance = np.cumsum(explained_variance_ratio)

# ============================================================
# STEP 5 â€” Plot Explained Variance (First 10 components)
# ============================================================
plt.figure(figsize=(12, 5))

# Bar chart for first 10 components
plt.subplot(1, 2, 1)
plt.bar(range(1, 11), explained_variance_ratio[:10], alpha=0.7, color='skyblue')
plt.xlabel('Principal Component')
plt.ylabel('Explained Variance Ratio')
plt.title('Explained Variance Ratio (First 10 Components)')

# Line plot for cumulative explained variance
plt.subplot(1, 2, 2)
plt.plot(range(1, len(cumulative_explained_variance)+1), cumulative_explained_variance, marker='o', color='orange')
plt.xlabel('Number of Components')
plt.ylabel('Cumulative Explained Variance')
plt.title('Cumulative Explained Variance')
plt.grid(True)

plt.tight_layout()
plt.show()

# ===============================
# STEP 1 â€” Import Libraries
# ===============================
import pandas as pd
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# ===============================
# STEP 2 â€” Load Breast Cancer Dataset
# ===============================
data = load_breast_cancer()
X = data.data
y = data.target

# Optional: Convert to DataFrame for better visualization
df = pd.DataFrame(X, columns=data.feature_names)
df['target'] = y
print(df.head())

# ===============================
# STEP 3 â€” Train/Test Split
# ===============================
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ===============================
# STEP 4 â€” Feature Scaling
# ===============================
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ===============================
# STEP 5 â€” Apply PCA
# ===============================
pca = PCA(n_components=0.95)  # retain 95% of variance
X_train_pca = pca.fit_transform(X_train_scaled)
X_test_pca = pca.transform(X_test_scaled)

print(f"Original shape: {X_train_scaled.shape}, PCA shape: {X_train_pca.shape}")

# ===============================
# STEP 6 â€” Train Classifiers
# ===============================
# Logistic Regression
lr = LogisticRegression(max_iter=1000)
lr.fit(X_train_pca, y_train)
y_pred_lr = lr.predict(X_test_pca)

# Random Forest
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train_pca, y_train)
y_pred_rf = rf.predict(X_test_pca)

# Support Vector Machine
svm = SVC(kernel='linear', probability=True)
svm.fit(X_train_pca, y_train)
y_pred_svm = svm.predict(X_test_pca)

# ===============================
# STEP 7 â€” Evaluation
# ===============================
def evaluate_model(name, y_true, y_pred):
    print(f"\n{name} Results:")
    print("Accuracy:", accuracy_score(y_true, y_pred))
    print("Classification Report:\n", classification_report(y_true, y_pred))
    print("Confusion Matrix:\n", confusion_matrix(y_true, y_pred))

evaluate_model("Logistic Regression", y_test, y_pred_lr)
evaluate_model("Random Forest", y_test, y_pred_rf)
evaluate_model("SVM", y_test, y_pred_svm)

# ==============================
# STEP 1 â€” Import Libraries
# ==============================
import pandas as pd
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# ==============================
# STEP 2 â€” Load Dataset
# ==============================
data = load_breast_cancer()
X = data.data
y = data.target

# ==============================
# STEP 3 â€” Train-Test Split
# ==============================
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# ==============================
# STEP 4 â€” Feature Scaling
# ==============================
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ==============================
# STEP 5 â€” Define Models
# ==============================
models = {
    "Logistic Regression": LogisticRegression(max_iter=10000, random_state=42),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),
    "SVM": SVC(kernel='linear', random_state=42)
}

# ==============================
# STEP 6 â€” Train, Predict, Evaluate
# ==============================
results = {}

for name, model in models.items():
    # Train
    model.fit(X_train_scaled, y_train)
    # Predict
    y_pred = model.predict(X_test_scaled)
    # Evaluate
    acc = accuracy_score(y_test, y_pred)
    cm = confusion_matrix(y_test, y_pred)
    cr = classification_report(y_test, y_pred)

    # Store results
    results[name] = {"Accuracy": acc, "Confusion Matrix": cm, "Classification Report": cr}

    # Display
    print(f"\n===== {name} =====")
    print(f"Test Accuracy: {acc:.4f}")
    print("Confusion Matrix:")
    print(cm)
    print("Classification Report:")
    print(cr)

# ==============================
# STEP 7 â€” Determine Best Model
# ==============================
best_model = max(results, key=lambda x: results[x]["Accuracy"])
print(f"\nBest performing model: {best_model}")
print("Reason: It has the highest test accuracy among the models tested.")

# ================================
# STEP 1 â€” Import Libraries
# ================================
import pandas as pd
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score
import matplotlib.pyplot as plt

# ================================
# STEP 2 â€” Load Dataset
# ================================
data = load_breast_cancer()
X = data.data
y = data.target

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Feature scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ================================
# STEP 3 â€” Define Models
# ================================
models = {
    "Logistic Regression": LogisticRegression(max_iter=10000),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),
    "K-Nearest Neighbors": KNeighborsClassifier(n_neighbors=5)
}

# ================================
# STEP 4 â€” Train, Predict & Confusion Matrix
# ================================
for name, model in models.items():
    # Train model
    model.fit(X_train_scaled, y_train)

    # Make predictions
    y_pred = model.predict(X_test_scaled)

    # Calculate accuracy
    acc = accuracy_score(y_test, y_pred)

    # Confusion matrix
    cm = confusion_matrix(y_test, y_pred)
    print(f"\n{name} - Accuracy: {acc:.4f}")
    print("Confusion Matrix:\n", cm)

    # Display confusion matrix visually
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=data.target_names)
    disp.plot(cmap=plt.cm.Blues)
    plt.title(f"{name} Confusion Matrix")
    plt.show()

# ==============================
# STEP 1 â€” Import Libraries
# ==============================
import pandas as pd
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report

# ==============================
# STEP 2 â€” Load Dataset
# ==============================
data = load_breast_cancer()
X = pd.DataFrame(data.data, columns=data.feature_names)
y = pd.Series(data.target)

# ==============================
# STEP 3 â€” Split Data
# ==============================
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ==============================
# STEP 4 â€” Standardize Features
# ==============================
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ==============================
# STEP 5 â€” Train Models
# ==============================
models = {
    "Logistic Regression": LogisticRegression(max_iter=10000),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),
    "SVM": SVC(kernel='linear', probability=True)
}

results = {}

for name, model in models.items():
    model.fit(X_train_scaled, y_train)
    y_pred = model.predict(X_test_scaled)
    acc = accuracy_score(y_test, y_pred)
    results[name] = acc
    print(f"{name} Accuracy: {acc:.4f}")

# ==============================
# STEP 6 â€” Compare Results
# ==============================
best_model = max(results, key=results.get)
print("\nBest Model:", best_model)
print("Reason: This model performs best because it achieved the highest accuracy on this dataset.")

# ==============================
# STEP 1 â€” Import Libraries
# ==============================
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_breast_cancer
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# ==============================
# STEP 2 â€” Load Dataset
# ==============================
data = load_breast_cancer()
X = data.data        # features
y = data.target      # labels
feature_names = data.feature_names
target_names = data.target_names

# Optional: convert to DataFrame for easier handling
df = pd.DataFrame(X, columns=feature_names)
df['target'] = y

# ==============================
# STEP 3 â€” Standardize Data
# ==============================
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# ==============================
# STEP 4 â€” Apply PCA
# ==============================
pca = PCA(n_components=2)  # first two principal components
X_pca = pca.fit_transform(X_scaled)

# Convert PCA result to DataFrame for easier plotting
df_pca = pd.DataFrame(X_pca, columns=['PC1', 'PC2'])
df_pca['target'] = y

# ==============================
# STEP 5 â€” Visualization
# ==============================
plt.figure(figsize=(10,6))

# Scatter plot
for target_value, target_name in enumerate(target_names):
    subset = df_pca[df_pca['target'] == target_value]
    plt.scatter(subset['PC1'], subset['PC2'], label=target_name, alpha=0.7)

plt.title('Breast Cancer Dataset - PCA 2D Scatter Plot')
plt.xlabel('Principal Component 1 (PC1)')
plt.ylabel('Principal Component 2 (PC2)')
plt.legend()
plt.grid(True)
plt.show()

# ===============================
# STEP 1 â€” Import Libraries
# ===============================
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# ===============================
# STEP 2 â€” Load Data
# ===============================
data = load_breast_cancer()
X, y = data.data, data.target

# ===============================
# STEP 3 â€” Split Data
# ===============================
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# ===============================
# STEP 4 â€” Scale Features
# ===============================
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ===============================
# STEP 5 â€” Reduce to 2D for visualization
# ===============================
pca = PCA(n_components=2)
X_train_pca = pca.fit_transform(X_train_scaled)
X_test_pca = pca.transform(X_test_scaled)

# ===============================
# STEP 6 â€” Train Classifiers
# ===============================
models = {
    "SVM (RBF)": SVC(kernel='rbf', probability=True, random_state=42),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),
    "KNN": KNeighborsClassifier(n_neighbors=5)
}

best_acc = 0
best_model_name = ""
best_model = None

for name, model in models.items():
    model.fit(X_train_pca, y_train)
    y_pred = model.predict(X_test_pca)
    acc = accuracy_score(y_test, y_pred)
    print(f"{name} Accuracy: {acc:.4f}")

    if acc > best_acc:
        best_acc = acc
        best_model_name = name
        best_model = model

# ===============================
# STEP 7 â€” Plot Decision Boundaries
# ===============================
# Create meshgrid
x_min, x_max = X_train_pca[:, 0].min() - 1, X_train_pca[:, 0].max() + 1
y_min, y_max = X_train_pca[:, 1].min() - 1, X_train_pca[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),
                     np.arange(y_min, y_max, 0.01))

# Predict for meshgrid
Z = best_model.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

# Plot
plt.figure(figsize=(10,6))
plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.coolwarm)
plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1], c=y_train, cmap=plt.cm.coolwarm, edgecolor='k', s=50)
plt.title(f"Decision Boundary of Best Model: {best_model_name}")
plt.xlabel("PCA Component 1")
plt.ylabel("PCA Component 2")
plt.show()

from joblib import dump, load

# Save the model
dump(model, "breast_cancer_model.pkl")
print("Model saved as breast_cancer_model.pkl")